# Guide de Tests MY-IA

Ce document rÃ©capitule l'infrastructure de tests complÃ¨te mise en place pour MY-IA.

## ğŸ“‹ Vue d'ensemble

L'infrastructure de tests comprend :

- **~100+ tests unitaires** pour l'API et les fonctions
- **Tests d'intÃ©gration end-to-end** avec services rÃ©els
- **CI/CD GitHub Actions** complet
- **Coverage minimum** de 70%
- **Mocking complet** pour isolation des tests

## ğŸ“ Structure

```
my-ia/
â”œâ”€â”€ pytest.ini                      # Configuration pytest
â”œâ”€â”€ requirements-test.txt           # DÃ©pendances de test
â”œâ”€â”€ run_tests.sh                    # Script de lancement
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â”œâ”€â”€ tests.yml              # Workflow CI/CD
â”‚       â””â”€â”€ README.md              # Documentation workflows
â””â”€â”€ tests/
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ conftest.py                # Fixtures globales
    â”œâ”€â”€ README.md                  # Guide des tests
    â”œâ”€â”€ INTEGRATION_TESTS.md       # Guide intÃ©gration
    â”œâ”€â”€ test_api_endpoints.py      # ~30 tests API
    â”œâ”€â”€ test_utility_functions.py  # ~20 tests utils
    â”œâ”€â”€ test_ingest.py             # ~30 tests ingestion
    â””â”€â”€ test_integration.py        # ~15 tests e2e
```

## ğŸš€ Installation

```bash
# Installer les dÃ©pendances de test
pip install -r requirements-test.txt

# VÃ©rifier l'installation
pytest --version
```

## ğŸ§ª ExÃ©cution des tests

### Tests rapides (unitaires)

```bash
# Avec le script
./run_tests.sh unit

# Ou directement
pytest -m unit -v
```

### Tests complets (avec intÃ©gration)

```bash
# DÃ©marrer les services
docker compose up -d ollama chroma

# Lancer tous les tests
./run_tests.sh

# Ou spÃ©cifiquement
pytest -v
```

### Tests avec coverage

```bash
./run_tests.sh coverage

# Ouvrir le rapport HTML
open htmlcov/index.html
```

### Tests spÃ©cifiques

```bash
# Un fichier
pytest tests/test_api_endpoints.py -v

# Une classe
pytest tests/test_api_endpoints.py::TestChatEndpoint -v

# Un test spÃ©cifique
pytest tests/test_api_endpoints.py::TestChatEndpoint::test_chat_with_valid_api_key -v

# Avec un pattern
pytest -k "chat" -v  # Tous les tests contenant "chat"
```

## ğŸ·ï¸ Markers

Les tests sont catÃ©gorisÃ©s avec des markers :

```bash
# Tests unitaires seulement
pytest -m unit

# Tests d'intÃ©gration seulement
pytest -m integration

# Tests API
pytest -m api

# Tests du systÃ¨me d'ingestion
pytest -m ingest

# Tests sans les lents
pytest -m "not slow"

# Combiner les markers
pytest -m "unit and api"
```

## ğŸ“Š Coverage

Le coverage minimum requis est de **70%**.

```bash
# GÃ©nÃ©rer le coverage
pytest --cov=app --cov-report=html --cov-report=term-missing

# VÃ©rifier le coverage actuel
pytest --cov=app --cov-report=term
```

**Fichiers exclus du coverage :**
- `tests/*`
- `*/venv/*`
- `*/__pycache__/*`

## ğŸ”„ CI/CD GitHub Actions

### Workflow automatique

Chaque push ou PR dÃ©clenche :

1. **Lint & Format** (1-2 min)
   - Black, isort, Flake8

2. **Unit Tests** (2-3 min)
   - Python 3.10, 3.11, 3.12
   - Coverage upload vers Codecov

3. **Integration Tests** (8-12 min)
   - ChromaDB + Ollama
   - ModÃ¨le tinyllama

4. **Docker Build** (4-6 min)
   - Images app & frontend

5. **Security Scan** (1-2 min)
   - Safety, Bandit

6. **Coverage Report** (2-3 min)
   - Rapport HTML
   - Commentaire sur PR

### Badge de statut

Ajouter au README.md :

```markdown
![Tests](https://github.com/VOTRE-USERNAME/my-ia/workflows/Tests/badge.svg)
```

## ğŸ“ Ã‰crire de nouveaux tests

### Test unitaire

```python
import pytest

@pytest.mark.unit
class TestMaFonction:
    def test_comportement_normal(self, client, test_api_key):
        """VÃ©rifie le comportement normal"""
        response = client.post(
            "/endpoint",
            json={"data": "test"},
            headers={"X-API-Key": test_api_key}
        )

        assert response.status_code == 200
        assert "expected_key" in response.json()
```

### Test d'intÃ©gration

```python
import pytest

@pytest.mark.integration
@pytest.mark.slow
@pytest.mark.asyncio
class TestWorkflowComplet:
    async def test_e2e(self, async_client, test_api_key):
        """Test du workflow complet"""
        # Skip si services non disponibles
        health = await async_client.get("/health")
        if not health.json().get("ollama"):
            pytest.skip("Ollama non disponible")

        # Test...
```

## ğŸ”§ Fixtures disponibles

### Clients
- `client` : TestClient FastAPI (sync)
- `async_client` : httpx.AsyncClient (async)

### DonnÃ©es
- `sample_chat_request` : RequÃªte chat valide
- `sample_document_text` : Document pour tests
- `sample_embeddings` : Vecteur d'embeddings
- `mock_chroma_results` : RÃ©sultats ChromaDB
- `mock_ollama_response` : RÃ©ponse Ollama

### Mocks
- `mock_ollama_embeddings` : Mock get_embeddings()
- `mock_chroma_search` : Mock search_context()
- `mock_ollama_generate` : Mock generate_response()

### Configuration
- `test_api_key` : ClÃ© API de test
- `test_ollama_host` : URL Ollama
- `test_chroma_host` : URL ChromaDB

## ğŸ› Debugging

### Afficher les prints

```bash
pytest -s
```

### Mode debug (pdb)

```bash
pytest --pdb
```

Ou dans le code :
```python
import pdb; pdb.set_trace()
```

### Logs dÃ©taillÃ©s

```bash
pytest --log-cli-level=DEBUG
```

### Re-runner le dernier Ã©chec

```bash
pytest --lf  # Last failed
pytest --ff  # Failed first
```

## ğŸ“ˆ MÃ©triques

### Statistiques actuelles

- **Tests totaux** : ~100+
- **Tests unitaires** : ~80
- **Tests d'intÃ©gration** : ~15
- **Coverage** : 70%+ (objectif)
- **DurÃ©e moyenne** : 3-5 min (unit), 10-15 min (all)

### Par fichier

| Fichier | Tests | Coverage | DurÃ©e |
|---------|-------|----------|-------|
| test_api_endpoints.py | ~30 | 85% | 1 min |
| test_utility_functions.py | ~20 | 80% | 1 min |
| test_ingest.py | ~30 | 75% | 1 min |
| test_integration.py | ~15 | N/A | 10 min |

## ğŸ”’ Best Practices

1. **Tests isolÃ©s** : Chaque test doit Ãªtre indÃ©pendant
2. **Noms descriptifs** : `test_chat_rejects_invalid_api_key`
3. **Une assertion principale** : Focus sur un comportement
4. **Utiliser les fixtures** : Ã‰viter la duplication
5. **Mocker les dÃ©pendances** : Isoler le code testÃ©
6. **Documenter les tests** : Docstrings claires
7. **Skip intelligemment** : `pytest.skip()` si services down

## ğŸš¨ ProblÃ¨mes courants

### Tests qui passent localement mais Ã©chouent en CI

**Solutions :**
- VÃ©rifier les chemins (absolu vs relatif)
- VÃ©rifier les variables d'environnement
- VÃ©rifier les versions Python
- VÃ©rifier les services (Ollama, ChromaDB)

### Timeout en tests d'intÃ©gration

**Solutions :**
```bash
# Augmenter le timeout
pytest --timeout=300

# Utiliser un modÃ¨le plus lÃ©ger
export MODEL_NAME="tinyllama"
```

### ChromaDB non accessible

**Solutions :**
```bash
# VÃ©rifier le conteneur
docker ps | grep chroma

# RedÃ©marrer
docker compose restart chroma

# VÃ©rifier les logs
docker compose logs chroma
```

## ğŸ“š Ressources

- [Guide des tests](tests/README.md)
- [Tests d'intÃ©gration](tests/INTEGRATION_TESTS.md)
- [Workflows CI/CD](.github/workflows/README.md)
- [Documentation pytest](https://docs.pytest.org/)
- [pytest-asyncio](https://pytest-asyncio.readthedocs.io/)

## ğŸ¯ Prochaines Ã©tapes

Pour amÃ©liorer encore les tests :

- [ ] Tests de performance (load testing)
- [ ] Tests de sÃ©curitÃ© (penetration testing)
- [ ] Tests frontend (Jest, Playwright)
- [ ] Tests E2E complets (Selenium)
- [ ] Mutation testing (coverage de qualitÃ©)
- [ ] Property-based testing (Hypothesis)

## ğŸ“ Support

En cas de problÃ¨me avec les tests :

1. Consulter les docs dans `tests/`
2. VÃ©rifier les logs CI/CD
3. Tester localement avec `-v -s`
4. Ouvrir une issue GitHub

---

**DerniÃ¨re mise Ã  jour** : 27 novembre 2025
**Auteur** : Claude (via Claude Code)
